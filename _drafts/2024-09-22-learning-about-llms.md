---
layout: post
title:  "Learning about LLMs"
categories:
    - AI, Machine Learning, Deep Learning
    - Learning
---

![Yellow lichen]({{ "/images/lichen.jpg" | absolute_url }}){:style="float: right; width: 55%; margin: 0em 0em 2em 2em;"}

Resources for learning about Large Language Models:

## LLM Agents MOOC

The [Large Language Model Agents MOOC, Fall 2024][2] led by [Dawn Song][3] Professor, UC Berkeley and [Xinyun Chen][4] Research Scientist, Google DeepMind.

- 12 lectures each with a quiz
- Written article assignment
- 3 lab assignments
- [LLM Agents Hackathon][5]


## Mastering LLMs

Led by Hamel Husain, [Mastering LLMs][1] was a [paid course][6] that ran earlier this year. They's released the videos on prompting, evals, RAG, find-tuning, tooling, and other topics.


## Sebastian Raschka

[Sebastian Raschka][8]'s latest book is [Build a Large Language Model From Scratch][7]. He also released [Building LLMs from the Ground Up: A 3-hour Coding Workshop][9].


## Papers

- [What Weâ€™ve Learned From A Year of Building with LLMs][105] A practical guide to building successful LLM products.
- [Large Language Models: A Survey][107] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao
- [A Primer On The Inner Workings Of Transformer-Based Language Models][101]
- [The Elements of Differentiable Programming][104] Mathieu Blondel, Vincent Roulet
- [Towards Conversational Diagnostic AI][109] AIME paper from Google



[1]: https://parlance-labs.com/education/
[2]: https://llmagents-learning.org/f24
[3]: https://dawnsong.io/
[4]: https://jungyhuk.github.io/
[5]: https://rdi.berkeley.edu/llm-agents-hackathon/
[6]: https://maven.com/parlance-labs/fine-tuning
[7]: https://www.manning.com/books/build-a-large-language-model-from-scratch
[8]: https://sebastianraschka.com/
[9]: https://www.youtube.com/watch?v=quh7z1q7-uc

[101]: https://arxiv.org/pdf/2405.00208
[104]: https://arxiv.org/abs/2403.14606
[105]: https://applied-llms.org/
[107]: https://arxiv.org/abs/2402.06196
[109]: https://arxiv.org/pdf/2401.05654
