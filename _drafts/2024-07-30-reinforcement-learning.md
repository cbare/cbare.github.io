---
layout: post
title:  "Reinforcement Learning"
categories: AI, Machine Learning, Deep Learning
---

Reinforcement learning is one of three branches of machine learning. It's useful when we don't have a clear label that we want our model to predict. Rather, we have a reward - a signal about how good or bad were the results of a particular action. For example, winning the game or making a profit is good. Making a bigger profit is better. Crashing the Tesla is bad.

<figure style="width: 90%; margin-left: 2em;">
    <a href="https://mathworks.com/discovery/reinforcement-learning.html" target="_blank">
    <img
      src="/images/three-branches-of-ml.png"
       alt="Three branches of ML"
       class="img-responsive"
      >
    </a>
    <figcaption style="font-size: small; font-style: italic; text-align: right; margin-right: 4em;">Mathworks - Reinforcement Learning</figcaption>
</figure>

## Applications

RL is typically applied in cases where sequential decisions have to made over time and those decisions affect future states and rewards,  where labeled examples come at significant cost, or where a scoring function is difficult to define.

- Robotics
- Autonomous vehicles
- Games
- Finance
- Alignment in LLMs ([RLHF][6])

### RL in games

RL has seen success in games, for example the victory of AlphaGo over the world champion Go player, Lee Sedol, and this paper which applies RL to beating Atari games:

<figure style="width: 90%; margin-left: 2em;">
    <a href="https://arxiv.org/pdf/1312.5602" target="_blank">
    <img
      src="/images/playing-atari-with-deep-rl.png"
       alt="Playing Atari with Deep Reinforcement Learning"
       class="img-responsive"
      >
    </a>
</figure>

Check out Andrej Karpathy's post [Deep Reinforcement Learning: Pong from Pixels][2].

<figure style="margin-left: 8em; margin-top: 2em;">
    <a href="https://karpathy.github.io/2016/05/31/rl/" target="_blank">
    <img
      src="/images/pong-from-pixels.png"
       alt="Pong from pixels"
       class="img-responsive"
      >
    </a>
</figure>


### RLHF

Recent success using [reinforcement learning with human feedback][6] (RLHF) to train large language models has renewed interest in RL. The diagram below from [Training language models to follow instructions with human feedback][8] (Ouyang et al, 2022) shows the training process for ChatGPT. Step 1 is supervised fine-tuning. Steps 2 and 3 are RLHF, first training a reward model on human preferences, then applying that reward model to modify the scoring of output generated by the LLM.

The insight is that it's much easier for a human labeler to rank outputs than to produce a high quality piece of training data. The ranking becomes the reward that helps the LLM learn to prefer the higher quality examples from the very mixed bag used during pretraining - giant text dumps of the whole internet.

<figure style="margin-left: 2em; margin-top: 2em;">
    <a href="https://arxiv.org/pdf/2203.02155" target="_blank">
    <img
      src="/images/openai-rlhf.png"
       alt="ChatGPT training process"
       class="img-responsive"
      >
    </a>
    <figcaption style="font-size: small; font-style: italic; text-align: right; margin-right: 4em;">RLHF - Ouyang et al, 2022, OpenAI</figcaption>
</figure>


## The RL problem

Reinforcement learning aims to “capture the most important aspects of the real problem facing a learning agent interacting over time with its environment to achieve a goal.” (Sutton and Barto) It's a problem definition doesn't assume a particular method for solution. Using neural networks to solve RL problems, as shown in the pong example above, is one method but not the only one.

The basic elements are **state**, **action**, and **reward**. The agent's goal is to choose the actions, given its observations of state, that maximize cumulative future reward.

<figure style="margin-left: 2em; margin-top: 2em;">
    <img
      src="/images/reinforcement-learning.png"
       alt="Reinforcement learning"
       class="img-responsive"
      >
    <figcaption style="font-size: small; font-style: italic; text-align: right; margin-right: 4em;">Audrey Durand - Exploration/Exploitation and Bandits</figcaption>
</figure>


### Adding complexity

But, it gets worse. Consider the uncertainty and incomplete information with which we must try to understand the world. We typically can't observe the complete state of the world and what we can observe is subject to misinterpretation. Was that a gunshot or just fireworks? Our actions may not have the hoped-for results and we suffer the consequences if we choose unwisely. Rewards may come later, well after a critical action was taken.

<figure style="margin-left: 2em; margin-top: 2em;">
    <img
      src="/images/reinforcement-learning-challenges.png"
       alt="Reinforcement learning challenges"
       class="img-responsive"
      >
    <figcaption style="font-size: small; font-style: italic; text-align: right; margin-right: 4em;">Audrey Durand - Exploration/Exploitation and Bandits</figcaption>
</figure>

If we assume the environment is unchanging, we can learn an optimal policy - the mapping between state and the next action - and sit back and collect rewards. But what if the dynamics of the environment change over time? Or if competing agents adapt their strategy to counter ours? That

### Explore/Exploit tradeoff

One important aspect of RL is the trade-off between exploration and exploitation. Exploration has a price.

> “To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to be e↵ective in producing reward. But to discover such actions, it has to try actions that it has not selected before. The agent has to exploit what it has already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future. The dilemma is that neither exploration nor exploitation can be pursued exclusively without failing at the task. The agent must try a variety of actions and progressively favor those that appear to be best. On a stochastic task, each action must be tried many times to gain a reliable estimate of its expected reward. The exploration–exploitation dilemma has been intensively studied by mathematicians for many decades, yet remains unresolved.” (Sutton and Barto)

<figure style="margin-left: 2em; margin-top: 2em;">
    <img
      src="/images/exploration-exploitation.png"
       alt="Exploration/exploitation tradeoff"
       class="img-responsive"
      >
    <figcaption style="font-size: small; font-style: italic; text-align: right; margin-right: 4em;">Audrey Durand - Exploration/Exploitation and Bandits</figcaption>
</figure>

## Terminology and definitions

- $$S$$ is a set of states
- $$A$$ is a set of actions
- $$R(s, s', a) \in \mathbb{R}$$ is the immediate reward after going from state $$s$$ to state $$s'$$ with action $$a$$.


### Policy

**policy** $$\pi$$ is a function $$\pi: S \rightarrow A$$

$$ \pi(A_t = a \mid S_t = s) $$

$$ \forall A_t \in \mathcal{A}(s), \forall S_t \in \mathcal{S} $$

### Value

$$V^\pi(s) = E_{\pi} \{G_t \vert s_t = s\} = E_\pi \{\sum_{k=0}^\infty \gamma^k r_{t+k+1} | s_t = s\}$$

$$G_t$$ is the total discounted reward from time step $$t$$

$$\gamma \in [0, 1]$$ is the discount factor


### Q

$$Q: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$$

$$Q^\pi(s, a)$$ is the "state action" value function, also known as the quality function. It is the expected return starting from state $$s$$, taking action $$a$$, then following policy $$\pi$$. It's focusing on the particular action at the particular state.

$$Q^\pi(s, a) = E_\pi \{G_t | s_t = s, a_t = a\}  = E_\pi \{\sum_{k=0}^\infty \gamma^k r_{t+k+1} | s_t = s, a_t=a\}$$

Q-learning is a model-free algorithm to learn the value of an action in a particular state.

## Resources

- [Reinforcement Learning: An Introduction][1] by Richard Sutton and Andrew Barto
- [Deep Reinforcement Learning: Pong from Pixels][2] by Andrej Karpathy
- [David Silver’s course][3] on RL
- John Schulman [Deep Reinforcement Learning][4] and [Policy Gradient Methods: Tutorial and New Frontiers][5]


[1]: http://incompleteideas.net/book/the-book-2nd.html
[2]: https://karpathy.github.io/2016/05/31/rl/
[3]: https://www.davidsilver.uk/teaching/
[4]: https://www.youtube.com/watch?v=oPGVsoBonLM
[5]: https://www.youtube.com/watch?v=y4ci8whvS1E
[6]: https://huyenchip.com/2023/05/02/rlhf.html
[7]: https://mathworks.com/discovery/reinforcement-learning.html
[8]: https://arxiv.org/pdf/2203.02155
[9]: https://arxiv.org/abs/2307.11046

