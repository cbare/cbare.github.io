---
layout: post
title:  "NYU Deep Learning Spring 2021"
date:   2021-11-21 08:47:00 +1200
categories: machine-learning deep-learning
---

NYU has generously made freely available all the lectures and materials from the Spring 2021 [Deep Learning Course][1] taught by [Yann LeCun][8] and [Alfredo Canziani][9].

### Class Materials

- [Deep Learning Course Website][1]
- [Lecture Videos on YouTube][2]
- [pytorch-Deep-Learning][3]
- [NYU-DLSP21][4]
- [][10]

### Background resources

- [Essence of Linear Algebra][5] by 3Blue1Brown
- [A 2020 Vision of Linear Algebra][6] six brief videos for teaching and learning linear algebra by Gilbert Strang
- [inear Algebra - Math for Machine Learning][7] from W&B


### Lectures

- [x] 01 – History and resources | 50:18
- [x] 01L – Gradient descent and the backpropagation algorithm | 1:51:04
- [ ] 02 – Neural nets: rotation and squashing | 1:01:54
- [ ] 02L – Modules and architectures | 1:42:27
- [ ] 03 – Tools, classification with neural nets, PyTorch implementation | 1:05:48
- [ ] 03L – Parameter sharing: recurrent and convolutional nets | 1:59:47
- [ ] 04L – ConvNet in practice | 51:41
- [ ] 04.1 – Natural signals properties and the convolution | 1:09:13
- [ ] 04.2 – Recurrent neural networks, vanilla and gated (LSTM) | 1:05:36
- [ ] 05L – Joint embedding method and latent variable energy based models (LV-EBMs) | 1:51:31
- [ ] 05.1 – Latent Variable Energy Based Models (LV-EBMs), inference | 1:01:05
- [ ] 05.2 – But what are these EBMs used for? | 10:42
- [ ] 06L – Latent variable EBMs for structured prediction | 1:48:54
- [ ] 06 – Latent Variable Energy Based Models (LV-EBMs), training | 1:04:49
- [ ] 07L – PCA, AE, K-means, Gaussian mixture model, sparse coding, and intuitive VAE | 1:54:23
- [ ] 07 – Unsupervised learning: autoencoding the targets | 56:42
- [ ] 08L – Self-supervised learning and variational inference | 1:54:44
- [ ] 08 – From LV-EBM to target prop to (vanilla, denoising, contractive, variational) autoencoder | 1:00:35
- [ ] 09L – Differentiable associative memories, attention, and transformers | 2:00:29
- [ ] 09 – AE, DAE, and VAE with PyTorch; generative adversarial networks (GAN) and code | 1:07:51
- [ ] 10L – Self-supervised learning in computer vision | 1:36:13
- [ ] 10 – Self / cross, hard / soft attention and the Transformer | 1:12:01
- [ ] 11L – Speech recognition and Graph Transformer Networks | 1:55:04
- [ ] 11 – Graph Convolutional Networks (GCNs) | 57:34
- [ ] 12L – Low resource machine translation | 1:57:56
- [ ] 12 – Planning and control | 1:10:23
- [ ] 13L – Optimisation for Deep Learning | 1:51:32
- [ ] 13 – The Truck Backer-Upper | 1:01:22
- [ ] 14L – Lagrangian backpropagation, final project winners, and Q&A session | 2:12:36
- [ ] 14 – Prediction and Planning Under Uncertainty | 1:14:45



[1]: https://cds.nyu.edu/deep-learning/
[2]: https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI
[3]: https://github.com/Atcold/pytorch-Deep-Learning
[4]: https://github.com/Atcold/NYU-DLSP21
[5]: https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab
[6]: https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/
[7]: https://www.youtube.com/watch?v=uZeDTwWcnuY&list=PLD80i8An1OEGZ2tYimemzwC3xqkU0jKUg
[8]: http://yann.lecun.com/
[9]: https://twitter.com/alfcnz
